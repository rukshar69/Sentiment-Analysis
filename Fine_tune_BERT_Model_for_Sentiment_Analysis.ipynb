{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQIJ+Q0AQepXh8mJv9/ogv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rukshar69/Sentiment-Analysis/blob/main/Fine_tune_BERT_Model_for_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jd-YSo9I4w7v"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "          split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "          as_supervised=True,\n",
        "          with_info=True)"
      ],
      "metadata": {
        "id": "_MZCBV0Q5Ey5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "hk3b5x8e-ijz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOL"
      ],
      "metadata": {
        "id": "Nfp7qpJzFU5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "id": "n8gTnWCaSLjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_example_to_feature(review):\n",
        "  return tokenizer.encode_plus(review,\n",
        "                add_special_tokens = True, # add [CLS], [SEP]\n",
        "                max_length = max_length, # max length of the text that can go to BERT\n",
        "                pad_to_max_length = True, # add [PAD] tokens\n",
        "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
        "              )\n",
        "  \n",
        "# can be up to 512 for BERT\n",
        "max_length = 512\n",
        "batch_size = 6\n",
        "\n",
        "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
        "  return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_masks,\n",
        "  }, label\n",
        "\n",
        "\n",
        "def encode_examples(ds, limit=-1):\n",
        "  # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
        "  input_ids_list = []\n",
        "  token_type_ids_list = []\n",
        "  attention_mask_list = []\n",
        "  label_list = []\n",
        "  if (limit > 0):\n",
        "      ds = ds.take(limit)\n",
        "  for review, label in tfds.as_numpy(ds):\n",
        "    bert_input = convert_example_to_feature(review.decode())\n",
        "    input_ids_list.append(bert_input['input_ids'])\n",
        "    token_type_ids_list.append(bert_input['token_type_ids'])\n",
        "    attention_mask_list.append(bert_input['attention_mask'])\n",
        "    label_list.append([label])\n",
        "  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
      ],
      "metadata": {
        "id": "PLx7g3pbSU6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train dataset\n",
        "ds_train_encoded = encode_examples(ds_train).shuffle(10000).batch(batch_size)\n",
        "# test dataset\n",
        "ds_test_encoded = encode_examples(ds_test).batch(batch_size)"
      ],
      "metadata": {
        "id": "WpkSYWIrShFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
        "learning_rate = 2e-5\n",
        "# we will do just 1 epoch, though multiple epochs might be better as long as we will not overfit the model\n",
        "number_of_epochs = 1\n",
        "# model initialization\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# choosing Adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
        "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "metadata": {
        "id": "G9sZubzdSjSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_history = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_test_encoded)"
      ],
      "metadata": {
        "id": "UxenTZ1QSj0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"This is a really good movie. I loved it and will watch again\"\n",
        "\n",
        "predict_input = tokenizer.encode(test_sentence,\n",
        "\n",
        "truncation=True,\n",
        "\n",
        "padding=True,\n",
        "\n",
        "return_tensors=\"tf\")\n",
        "\n",
        "tf_output = model.predict(predict_input)[0]\n",
        "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
        "labels = ['Negative','Positive'] #(0:negative, 1:positive)\n",
        "label = tf.argmax(tf_prediction, axis=1)\n",
        "label = label.numpy()\n",
        "print(labels[label[0]])"
      ],
      "metadata": {
        "id": "HD2LplAwSodb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WKOYWWr9SqfR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}